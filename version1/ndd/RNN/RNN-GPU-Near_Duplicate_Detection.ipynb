{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please find the 3 correct and 3 incorrect predictions at the bottom of this notebook. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import io\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for each step in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastText = []\n",
    "with open('wiki-news-300d-1M.vec', \"r\") as ft:\n",
    "    for i, line in enumerate(ft):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        FastText.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def build_embedding(data):    \n",
    "    word2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    id2word = {0: \"<pad>\", 1: \"<unk>\"}\n",
    "    \n",
    "    embeddings = [np.zeros(300),np.random.normal(0, 0.01, 300)]\n",
    "    \n",
    "    for i, line in enumerate(data):\n",
    "        parsed = line.split()\n",
    "        word = parsed[0]\n",
    "        array = np.array([float(x) for x in parsed[1:]])\n",
    "    \n",
    "        word2id[word] = i+2\n",
    "        id2word[i+2] = word\n",
    "        embeddings.append(array)\n",
    "        \n",
    "    \n",
    "    return word2id, id2word, embeddings\n",
    " \n",
    "token2id, id2token, word_vectors = build_embedding(FastText)\n",
    "BATCH_SIZE = 64\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integers(data_label):\n",
    "    for i in range(len(data_label)):\n",
    "        if data_label[i] == 0:\n",
    "            data_label[i] = 0\n",
    "        elif data_label[i] == 1:\n",
    "            data_label[i] = 1\n",
    "    return data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_order(sent1_data, sent2_data, data_label):\n",
    "    i = random.randint(1, len(sent1_data))\n",
    "    print(sent1_data[i])\n",
    "    print(sent2_data[i])\n",
    "    print(data_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def tokenize(sentence_list):\n",
    "    d = collections.defaultdict(int)\n",
    "    for i in range(len(sentence_list)):\n",
    "        d[type(sentence_list[i])]+=1\n",
    "    print(d)\n",
    "    filtered_sentences = [sent for sent in sentence_list if isinstance(sent, (float, int))]\n",
    "    print(filtered_sentences)\n",
    "    return [word_tokenize(str(sentence_list[i])) for i in range(len(sentence_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"one-hot encode\": convert each token to id in vocabulary vector (token2id)\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary & embedding matrix from FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors, token2id, id2token = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999996, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weights = np.array(word_vectors)\n",
    "_WEIGHTS = _weights\n",
    "_WEIGHTS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pre-process data for TwoSentenceModel\n",
    "#### Shuffle, word tokenize, one-hot index into vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(sent1s, sent2s, labels, verify=True):\n",
    "    labels = convert_labels_to_integers(labels)\n",
    "    seed = random.randint(1, 100)\n",
    "    print(\"Random seed for shuffling: {}\".format(seed))\n",
    "    random.Random(seed).shuffle(sent1s)\n",
    "    random.Random(seed).shuffle(sent2s)\n",
    "    random.Random(seed).shuffle(labels)\n",
    "    \n",
    "    print(\"\\nVerifying that the data and label match after shuffling\")\n",
    "    if verify:\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "    print(len(sent1s)) \n",
    "    print((sent1s[-1]))\n",
    "    print(\"\\nTokenizing sentence 1 list...\")    \n",
    "    sent1s_tokenized = tokenize(sent1s)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nTokenizing sentence 2 list... \")  \n",
    "    sent2s_tokenized = tokenize(sent2s)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    print(\"\\nOne-hot encoding words for sentence 1 list...\")  \n",
    "    sent1s_indices = token2index_dataset(sent1s_tokenized)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nOne-hot encoding words for sentence 2 list...\")  \n",
    "    sent2s_indices = token2index_dataset(sent2s_tokenized)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    return (sent1s_indices, sent2s_indices, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SENTENCE_LENGTH = 30\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TwoSentencesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_data_list, sent2_data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param sent1_data_list: list of sentence1's (index matches sentence2's and target_list below)\n",
    "        @param sent2_data_list: list of sentence2's\n",
    "        @param target_list: list of correct labels\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_data_list = sent1_data_list\n",
    "        self.sent2_data_list = sent2_data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_data_list) == len(self.target_list) and len(self.sent2_data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1_data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        ###\n",
    "        ### Returns [[sentence, 1, tokens], [sentence, 2, tokens]]\n",
    "        ###\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1_tokens_idx = self.sent1_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_tokens_idx = self.sent2_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        combined_tokens_idx = [sent1_tokens_idx, sent2_tokens_idx]\n",
    "        label = self.target_list[key]\n",
    "        return [combined_tokens_idx, len(sent1_tokens_idx), len(sent2_tokens_idx), label]\n",
    "\n",
    "def twosentences_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_data_list = []\n",
    "    sent2_data_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_length_list = []\n",
    "    label_list = []\n",
    "    padded_vec = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[3])\n",
    "        sent1_length_list.append(datum[1])\n",
    "        sent2_length_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec.append(np.pad(np.concatenate((np.array(datum[0][0]),np.array(datum[0][1]))), pad_width=((0,2*MAX_SENTENCE_LENGTH-datum[1]-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0))\n",
    "        \n",
    "    return [torch.from_numpy(np.array(padded_vec)), \n",
    "            torch.LongTensor(sent1_length_list), torch.LongTensor(sent2_length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial dataset size 404351\n",
      "Training set size: 323480\n",
      "Validation set size: 40435\n",
      "Test set size: 40436\n",
      "<class 'int'>\n",
      "What's the best customer service experience you've ever had?\n",
      "What was your worst customer service experience you've ever had?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "file_path = './QQP/questions.csv'\n",
    "dataframe = pd.read_csv(file_path)\n",
    "sentence1_list = dataframe['question1'].tolist()\n",
    "sentence2_list = dataframe['question2'].tolist()\n",
    "target_label_list = dataframe['is_duplicate'].tolist()\n",
    "\n",
    "print(\"initial dataset size\", len(sentence1_list))\n",
    "\n",
    "# First split: train+val and test (10% of the original data for test set)\n",
    "sentence1_train, sent1_test, sentence2_train, sent2_test, target_train, test_label = train_test_split(\n",
    "    sentence1_list, sentence2_list, target_label_list, test_size=0.1, random_state=42)\n",
    "\n",
    "# Second split: train and validation (10% of the original data for validation set, which is 1/9th of the remaining 90%)\n",
    "sent1_data, sent1_val, sent2_data, sent2_val, data_label, val_label = train_test_split(\n",
    "    sentence1_train, sentence2_train, target_train, test_size=0.1/0.9, random_state=42)\n",
    "\n",
    "# Output the sizes of each set\n",
    "print(f\"Training set size: {len(sent1_data)}\")\n",
    "print(f\"Validation set size: {len(sent1_val)}\")\n",
    "print(f\"Test set size: {len(sent1_test)}\")\n",
    "print(type(val_label[40]))\n",
    "print(sent2_val[40])\n",
    "print(sent1_val[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 70\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "How do I change incorrect address in Chennai property tax document?\n",
      "What does = and == signifies in c++ programming language?\n",
      "0\n",
      "Is it possible for antibiotics to cause acne?\n",
      "How could antibiotics cause acne?\n",
      "1\n",
      "323480\n",
      "Which is the best movie download site?\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "defaultdict(<class 'int'>, {<class 'str'>: 323479, <class 'float'>: 1})\n",
      "[nan]\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "defaultdict(<class 'int'>, {<class 'str'>: 323479, <class 'float'>: 1})\n",
      "[nan]\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Finished creating train_loader.\n"
     ]
    }
   ],
   "source": [
    "sent1_train_indices, sent2_train_indices, train_label = data_pipeline(sent1_data, sent2_data, data_label)\n",
    "train_dataset = TwoSentencesDataset(sent1_train_indices, sent2_train_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )\n",
    "print(\"Finished creating train_loader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = max(max([len(sent) for sent in sent1_train_indices]), max([len(sent) for sent in sent2_train_indices]))\n",
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 8\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "How does someone reconcile being \"fiscally conservative\" and \"socially liberal\"?\n",
      "What are some major socially liberal, fiscally conservative political parties?\n",
      "0\n",
      "What are the strongest majors in terms of job prospects and what are the weakest majors at Chicago State?\n",
      "What are the strongest majors in terms of job prospects and what are the weakest majors at Montclair State?\n",
      "0\n",
      "40435\n",
      "How is Modi different from Kejriwal?\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "defaultdict(<class 'int'>, {<class 'str'>: 40435})\n",
      "[]\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "defaultdict(<class 'int'>, {<class 'str'>: 40434, <class 'float'>: 1})\n",
      "[nan]\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "sent1_val_indices, sent2_val_indices, val_label = data_pipeline(sent1_val, sent2_val, val_label)\n",
    "val_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 24\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "How many hits does the search \"end times\" have in Google?\n",
      "How does one detect forward edges in a DFS tree using start and end times?\n",
      "0\n",
      "Will Donald Trump beat Hillary and become our next president?\n",
      "Who's going to win and become president, Hillary Clinton or Donald Trump?\n",
      "1\n",
      "40436\n",
      "Subway has a \"Perfect Bell\" to ring when your sandwich is just right. What do employees and customers really think of it?\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "defaultdict(<class 'int'>, {<class 'str'>: 40436})\n",
      "[]\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "defaultdict(<class 'int'>, {<class 'str'>: 40436})\n",
      "[]\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "sent1_test_indices, sent2_test_indices, test_label = data_pipeline(sent1_test, sent2_test, test_label)\n",
    "test_dataset = TwoSentencesDataset(sent1_test_indices, sent2_test_indices, test_label)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoSentenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_layers, num_classes, emb_size = 300):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(TwoSentenceModel, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        weight = torch.FloatTensor(_WEIGHTS)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.randn(2, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x, sent1_lengths, sent2_lengths):\n",
    "        # reset hidden state\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        ordered_slengths = sent1_lengths + sent2_lengths\n",
    "        \n",
    "    \n",
    "        reverse_sorted_lengths, reverse_sorted_indices = torch.sort(ordered_slengths, descending=True)\n",
    "        reverse_sorted_lengths = reverse_sorted_lengths.to(x.device)\n",
    "        reverse_sorted_lengths = reverse_sorted_lengths.cpu().numpy()\n",
    "        ordered_sents = x\n",
    "        reverse_sorted_data = ordered_sents[reverse_sorted_indices].to(device)\n",
    "        # get embedding\n",
    "        embed = self.embedding(reverse_sorted_data)\n",
    "        \n",
    "        \n",
    "\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, reverse_sorted_lengths, batch_first=True)\n",
    "            \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        # fprop though RNN\n",
    "        rnn_out, self.hidden = self.rnn(embed, self.hidden)\n",
    "        \n",
    "        ### MATCHING BACK\n",
    "        \n",
    "        change_back_indices = reverse_sorted_indices.argsort()\n",
    "        self.hidden = self.hidden[:, change_back_indices]\n",
    "              \n",
    "        ### GRU stuff\n",
    "        hidden_sents = torch.cat([self.hidden[0, :, :], self.hidden[1, :, :]], dim=1)\n",
    "        linear1 = self.linear1(hidden_sents)\n",
    "        linear1 = F.relu(linear1.contiguous().view(-1, linear1.size(-1))).view(linear1.shape)   \n",
    "        linear1 = self.dropout(linear1)\n",
    "        logits = self.linear2(linear1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Helper function that tests the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for (data, sent1_lengths, sent2_lengths, labels) in loader:\n",
    "        data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.to(device), sent1_lengths.to(device), sent2_lengths.to(device), labels.to(device)\n",
    "        outputs = F.softmax(model(data_batch, sent1_length_batch, sent2_length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        labels = labels.to(device)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(model, lr = 0.001, num_epochs = 30, criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    max_val_acc = 0\n",
    "    losses = []\n",
    "    xs = 0\n",
    "    val_accs = []\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, sent1_lengths, sent2_lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.to(device), sent1_lengths.to(device), sent2_lengths.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, sent1_length_batch, sent2_length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                val_accs.append(val_acc)\n",
    "                xs += 1000\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss))\n",
    "\n",
    "        file_name = f\"RNN_NDD/rnn_model_{epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), file_name)\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        test_acc = test_model(test_loader, model)\n",
    "        print(f\"test accuracy at epoch {epoch+1} is {test_acc}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "            # Save the model if validation accuracy improves\n",
    "            torch.save(model.state_dict(), \"RNN_NDD/BestModel/rnn_model.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                print(f\"Validation accuracy didn't improve for {patience} epochs. Early stopping at {epoch+1}\")\n",
    "                break\n",
    "                \n",
    "    print(\"Max Validation Accuracy: {}\".format(max_val_acc))\n",
    "    return max_val_acc, losses, xs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Step: [1001/5055], Validation Acc: 71.08940274514653\n",
      "Epoch: [1/30], Step: [1001/5055], Training Loss: 0.5338836312294006\n",
      "Epoch: [1/30], Step: [2001/5055], Validation Acc: 72.05144058365278\n",
      "Epoch: [1/30], Step: [2001/5055], Training Loss: 0.56545090675354\n",
      "Epoch: [1/30], Step: [3001/5055], Validation Acc: 71.95251638432052\n",
      "Epoch: [1/30], Step: [3001/5055], Training Loss: 0.5256136059761047\n",
      "Epoch: [1/30], Step: [4001/5055], Validation Acc: 74.32175095832818\n",
      "Epoch: [1/30], Step: [4001/5055], Training Loss: 0.4855661690235138\n",
      "Epoch: [1/30], Step: [5001/5055], Validation Acc: 75.09583281810313\n",
      "Epoch: [1/30], Step: [5001/5055], Training Loss: 0.5817773342132568\n",
      "test accuracy at epoch 1 is 75.16074784845188\n",
      "Epoch: [2/30], Step: [1001/5055], Validation Acc: 75.59787312971436\n",
      "Epoch: [2/30], Step: [1001/5055], Training Loss: 0.47598573565483093\n",
      "Epoch: [2/30], Step: [2001/5055], Validation Acc: 75.47174477556572\n",
      "Epoch: [2/30], Step: [2001/5055], Training Loss: 0.41433531045913696\n",
      "Epoch: [2/30], Step: [3001/5055], Validation Acc: 76.3744280944726\n",
      "Epoch: [2/30], Step: [3001/5055], Training Loss: 0.4612923562526703\n",
      "Epoch: [2/30], Step: [4001/5055], Validation Acc: 77.73710894027451\n",
      "Epoch: [2/30], Step: [4001/5055], Training Loss: 0.37933647632598877\n",
      "Epoch: [2/30], Step: [5001/5055], Validation Acc: 78.3356003462347\n",
      "Epoch: [2/30], Step: [5001/5055], Training Loss: 0.5261248350143433\n",
      "test accuracy at epoch 2 is 78.4597883074488\n",
      "Epoch: [3/30], Step: [1001/5055], Validation Acc: 78.72882403858044\n",
      "Epoch: [3/30], Step: [1001/5055], Training Loss: 0.4454203248023987\n",
      "Epoch: [3/30], Step: [2001/5055], Validation Acc: 78.00915048843824\n",
      "Epoch: [3/30], Step: [2001/5055], Training Loss: 0.33951613306999207\n",
      "Epoch: [3/30], Step: [3001/5055], Validation Acc: 78.67936193891431\n",
      "Epoch: [3/30], Step: [3001/5055], Training Loss: 0.4645525813102722\n",
      "Epoch: [3/30], Step: [4001/5055], Validation Acc: 79.44602448373934\n",
      "Epoch: [3/30], Step: [4001/5055], Training Loss: 0.2906591296195984\n",
      "Epoch: [3/30], Step: [5001/5055], Validation Acc: 79.72548534685298\n",
      "Epoch: [3/30], Step: [5001/5055], Training Loss: 0.4271897077560425\n",
      "test accuracy at epoch 3 is 79.6567415174597\n",
      "Epoch: [4/30], Step: [1001/5055], Validation Acc: 79.81946333621862\n",
      "Epoch: [4/30], Step: [1001/5055], Training Loss: 0.4171223044395447\n",
      "Epoch: [4/30], Step: [2001/5055], Validation Acc: 78.9464572771114\n",
      "Epoch: [4/30], Step: [2001/5055], Training Loss: 0.3009849488735199\n",
      "Epoch: [4/30], Step: [3001/5055], Validation Acc: 79.36441201929021\n",
      "Epoch: [4/30], Step: [3001/5055], Training Loss: 0.39647483825683594\n",
      "Epoch: [4/30], Step: [4001/5055], Validation Acc: 80.14591319401508\n",
      "Epoch: [4/30], Step: [4001/5055], Training Loss: 0.26477551460266113\n",
      "Epoch: [4/30], Step: [5001/5055], Validation Acc: 80.48720168171138\n",
      "Epoch: [4/30], Step: [5001/5055], Training Loss: 0.33096247911453247\n",
      "test accuracy at epoch 4 is 80.50499554852112\n",
      "Epoch: [5/30], Step: [1001/5055], Validation Acc: 80.54160999134413\n",
      "Epoch: [5/30], Step: [1001/5055], Training Loss: 0.4220702052116394\n",
      "Epoch: [5/30], Step: [2001/5055], Validation Acc: 79.95795721528378\n",
      "Epoch: [5/30], Step: [2001/5055], Training Loss: 0.26067379117012024\n",
      "Epoch: [5/30], Step: [3001/5055], Validation Acc: 80.01731173488315\n",
      "Epoch: [5/30], Step: [3001/5055], Training Loss: 0.34478020668029785\n",
      "Epoch: [5/30], Step: [4001/5055], Validation Acc: 80.78397427970818\n",
      "Epoch: [5/30], Step: [4001/5055], Training Loss: 0.2543827295303345\n",
      "Epoch: [5/30], Step: [5001/5055], Validation Acc: 80.6430072956597\n",
      "Epoch: [5/30], Step: [5001/5055], Training Loss: 0.3016227185726166\n",
      "test accuracy at epoch 5 is 80.66821644079533\n",
      "Epoch: [6/30], Step: [1001/5055], Validation Acc: 80.7938666996414\n",
      "Epoch: [6/30], Step: [1001/5055], Training Loss: 0.3758303225040436\n",
      "Epoch: [6/30], Step: [2001/5055], Validation Acc: 80.70483492024236\n",
      "Epoch: [6/30], Step: [2001/5055], Training Loss: 0.2339375615119934\n",
      "Epoch: [6/30], Step: [3001/5055], Validation Acc: 79.46828242858909\n",
      "Epoch: [6/30], Step: [3001/5055], Training Loss: 0.3305578827857971\n",
      "Epoch: [6/30], Step: [4001/5055], Validation Acc: 80.72461976010882\n",
      "Epoch: [6/30], Step: [4001/5055], Training Loss: 0.2029520571231842\n",
      "Epoch: [6/30], Step: [5001/5055], Validation Acc: 81.00902683318907\n",
      "Epoch: [6/30], Step: [5001/5055], Training Loss: 0.2560670077800751\n",
      "test accuracy at epoch 6 is 80.69789296666337\n",
      "Epoch: [7/30], Step: [1001/5055], Validation Acc: 81.32063806108569\n",
      "Epoch: [7/30], Step: [1001/5055], Training Loss: 0.28067100048065186\n",
      "Epoch: [7/30], Step: [2001/5055], Validation Acc: 80.99666130827254\n",
      "Epoch: [7/30], Step: [2001/5055], Training Loss: 0.21990925073623657\n",
      "Epoch: [7/30], Step: [3001/5055], Validation Acc: 79.91838753555088\n",
      "Epoch: [7/30], Step: [3001/5055], Training Loss: 0.24440895020961761\n",
      "Epoch: [7/30], Step: [4001/5055], Validation Acc: 80.92988747372326\n",
      "Epoch: [7/30], Step: [4001/5055], Training Loss: 0.2045745849609375\n",
      "Epoch: [7/30], Step: [5001/5055], Validation Acc: 81.26870285643625\n",
      "Epoch: [7/30], Step: [5001/5055], Training Loss: 0.20343060791492462\n",
      "test accuracy at epoch 7 is 80.8561677712929\n",
      "Epoch: [8/30], Step: [1001/5055], Validation Acc: 81.00902683318907\n",
      "Epoch: [8/30], Step: [1001/5055], Training Loss: 0.300165593624115\n",
      "Epoch: [8/30], Step: [2001/5055], Validation Acc: 81.09063929763819\n",
      "Epoch: [8/30], Step: [2001/5055], Training Loss: 0.17832189798355103\n",
      "Epoch: [8/30], Step: [3001/5055], Validation Acc: 79.88129096080128\n",
      "Epoch: [8/30], Step: [3001/5055], Training Loss: 0.2956802546977997\n",
      "Epoch: [8/30], Step: [4001/5055], Validation Acc: 80.86064053419068\n",
      "Epoch: [8/30], Step: [4001/5055], Training Loss: 0.16461892426013947\n",
      "Epoch: [8/30], Step: [5001/5055], Validation Acc: 81.07085445777173\n",
      "Epoch: [8/30], Step: [5001/5055], Training Loss: 0.27811577916145325\n",
      "test accuracy at epoch 8 is 80.78939558808982\n",
      "Epoch: [9/30], Step: [1001/5055], Validation Acc: 81.04859651292197\n",
      "Epoch: [9/30], Step: [1001/5055], Training Loss: 0.23293453454971313\n",
      "Epoch: [9/30], Step: [2001/5055], Validation Acc: 80.85074811425746\n",
      "Epoch: [9/30], Step: [2001/5055], Training Loss: 0.20931650698184967\n",
      "Epoch: [9/30], Step: [3001/5055], Validation Acc: 80.33634227772968\n",
      "Epoch: [9/30], Step: [3001/5055], Training Loss: 0.2212061733007431\n",
      "Epoch: [9/30], Step: [4001/5055], Validation Acc: 81.02881167305551\n",
      "Epoch: [9/30], Step: [4001/5055], Training Loss: 0.14211487770080566\n",
      "Epoch: [9/30], Step: [5001/5055], Validation Acc: 80.86558674415728\n",
      "Epoch: [9/30], Step: [5001/5055], Training Loss: 0.17750343680381775\n",
      "test accuracy at epoch 9 is 80.88089820951627\n",
      "Epoch: [10/30], Step: [1001/5055], Validation Acc: 80.6430072956597\n",
      "Epoch: [10/30], Step: [1001/5055], Training Loss: 0.18428170680999756\n",
      "Epoch: [10/30], Step: [2001/5055], Validation Acc: 80.89526400395697\n",
      "Epoch: [10/30], Step: [2001/5055], Training Loss: 0.17985907196998596\n",
      "Epoch: [10/30], Step: [3001/5055], Validation Acc: 80.46247063187832\n",
      "Epoch: [10/30], Step: [3001/5055], Training Loss: 0.18281318247318268\n",
      "Epoch: [10/30], Step: [4001/5055], Validation Acc: 80.89279089897366\n",
      "Epoch: [10/30], Step: [4001/5055], Training Loss: 0.10332732647657394\n",
      "Epoch: [10/30], Step: [5001/5055], Validation Acc: 80.86064053419068\n",
      "Epoch: [10/30], Step: [5001/5055], Training Loss: 0.1497056782245636\n",
      "test accuracy at epoch 10 is 80.92293995449599\n",
      "Epoch: [11/30], Step: [1001/5055], Validation Acc: 81.17967107703723\n",
      "Epoch: [11/30], Step: [1001/5055], Training Loss: 0.15592911839485168\n",
      "Epoch: [11/30], Step: [2001/5055], Validation Acc: 81.3602077408186\n",
      "Epoch: [11/30], Step: [2001/5055], Training Loss: 0.10896382480859756\n",
      "Epoch: [11/30], Step: [3001/5055], Validation Acc: 79.80709781130209\n",
      "Epoch: [11/30], Step: [3001/5055], Training Loss: 0.15202680230140686\n",
      "Epoch: [11/30], Step: [4001/5055], Validation Acc: 80.88537158402374\n",
      "Epoch: [11/30], Step: [4001/5055], Training Loss: 0.13449598848819733\n",
      "Epoch: [11/30], Step: [5001/5055], Validation Acc: 81.17719797205392\n",
      "Epoch: [11/30], Step: [5001/5055], Training Loss: 0.16088271141052246\n",
      "test accuracy at epoch 11 is 80.7943416757345\n",
      "Validation accuracy didn't improve for 3 epochs. Early stopping at 11\n",
      "Max Validation Accuracy: 81.3602077408186\n"
     ]
    }
   ],
   "source": [
    "model = TwoSentenceModel(emb_size = 300, hidden_size=300, num_layers=1, num_classes=2).to(device)\n",
    "max_val_acc, losses, xs, val_accs = train_model(model, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
